{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1422e599-4fa5-430a-95fe-5902e3da4531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6049312-995a-47a3-b60b-0ff921011fca",
   "metadata": {},
   "source": [
    "## Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5adb3d1f-471a-4ac8-b4b5-b28fb574380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron(object):\n",
    "    \"\"\"\n",
    "    Process an input vector, return activation\n",
    "    Args:\n",
    "        num_inputs (int) : input size\n",
    "        activation_function (callable) : activation function\n",
    "    Attributes:\n",
    "        W (array): weight values for each input\n",
    "        b (float): bias\n",
    "        activation_function (callable): activation function\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs, activation_function):\n",
    "        super().__init__()\n",
    "\n",
    "        self.W = np.random.uniform(size=num_inputs, low=-1., high=1.)\n",
    "        self.b = np.random.uniform(size=1, low=-1., high=1.)\n",
    "\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute input through neuron, return the activation value\n",
    "        Args : \n",
    "            x (ndarray): input vector. shape must be (1, num_inputs)\n",
    "        Returns:\n",
    "            activation (ndarray): activation value, of shape (1, layer_size)\n",
    "        \"\"\"\n",
    "        z = np.dot(x, self.W) + self.b\n",
    "        return self.activation_function(z)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f66654-4ae2-49b9-930e-6932d5929f64",
   "metadata": {},
   "source": [
    "Adding step function to have a perceptron : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "77fa92cf-64f0-4e3e-81f7-708cec8be515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron's random weights = [ 0.09342056 -0.63029109  0.93916926  0.55026565  0.87899788  0.7896547 ] , and random bias = [0.19579996]\n"
     ]
    }
   ],
   "source": [
    "input_size = 6\n",
    "\n",
    "step_function = lambda y: 0 if y <= 0 else 1\n",
    "\n",
    "perceptron = Neuron(num_inputs=input_size, activation_function=step_function)\n",
    "print(\"Perceptron's random weights = {} , and random bias = {}\".format(perceptron.W, perceptron.b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fd58fb8c-1f92-4e89-a018-bf78f8c92c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vector : [[0.92187424 0.0884925  0.19598286 0.04522729 0.32533033 0.38867729]]\n"
     ]
    }
   ],
   "source": [
    "# random input vector : \n",
    "x = np.random.rand(input_size).reshape(1, input_size)\n",
    "print(\"Input vector : {}\".format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "be9aa04d-a0da-478d-9003-f301be4f002a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron's output value given `x` : 1\n"
     ]
    }
   ],
   "source": [
    "y = perceptron.forward(x)\n",
    "print(\"Perceptron's output value given `x` : {}\".format(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb530396-36aa-4993-b18b-c9392ef19a16",
   "metadata": {},
   "source": [
    "## Fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8d9d1d9f-9be7-485f-bb0a-4c41e6bb9ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer(object):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        num_inputs (int): input vector size .\n",
    "        layer_size (int): number of neurons in the layer.\n",
    "        activation_function (callable): activation function for this layer.\n",
    "    Attributes:\n",
    "        W (ndarray): The weight values for each input.\n",
    "        b (ndarray): The bias value\n",
    "        size (int): The layer size / number of neurons.\n",
    "        activation_function (callable): The activation function computing the neuron's output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs, layer_size, activation_function):\n",
    "        super().__init__()\n",
    "\n",
    "        # Randomly initializing the weight vector and the bias value (using a normal distribution this time):\n",
    "        self.W = np.random.standard_normal((num_inputs, layer_size))\n",
    "        self.b = np.random.standard_normal(layer_size)\n",
    "        self.size = layer_size\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward the input vector through the layer\n",
    "        \"\"\"\n",
    "        z = np.dot(x, self.W) + self.b\n",
    "        return self.activation_function(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "48a69b86-38f4-48e7-bb07-48d07884e990",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.random.uniform(-1,1,2).reshape(1,2)\n",
    "x2 = np.random.uniform(-1,1,2).reshape(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e407b83e-b439-4fe2-92b8-d7d2264e85e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_fn = lambda y: np.maximum(y, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a8d9a4bd-2c26-43da-8bd5-165f4b3c8860",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = FullyConnectedLayer(2, 3, relu_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "def08e44-486a-4b10-b085-68c36c266001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.92846182 4.10050346 0.        ]]\n",
      "[[0.97876643 3.70290315 0.21685685]]\n"
     ]
    }
   ],
   "source": [
    "out1 = layer.forward(x1)\n",
    "out2 = layer.forward(x2)\n",
    "\n",
    "print(out1)\n",
    "print(out2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fafa17-5b78-423f-a9cd-a44d892780b9",
   "metadata": {},
   "source": [
    "## Simple Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ddcdd6-d330-4490-b077-c82c39da89ab",
   "metadata": {},
   "source": [
    "Let's take our fully connected layer and add backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "adbf74ab-c9a5-46c2-a6a4-e00956be18a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer(object):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        num_inputs (int): input vector size .\n",
    "        layer_size (int): number of neurons in the layer.\n",
    "        activation_function (callable): activation function for this layer.\n",
    "    Attributes:\n",
    "        W (ndarray): The weight values for each input.\n",
    "        b (ndarray): The bias value\n",
    "        size (int): The layer size / number of neurons.\n",
    "        activation_function (callable): The activation function computing the neuron's output.\n",
    "\n",
    "        x (ndarray): last provided input vector, stored for backpropagation\n",
    "        y (ndarray): corresponding output, stored for backpropagation\n",
    "        derivated_activation_function (callable): corresponding derivated activation function\n",
    "        dL_dW (ndarray): derivative of loss with respect to W\n",
    "        dL_db (ndarray): derivative of loss with respect to b\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs, layer_size, activation_function, derivated_activation_function=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # Randomly initializing the weight vector and the bias value (using a normal distribution this time):\n",
    "        self.W = np.random.standard_normal((num_inputs, layer_size))\n",
    "        self.b = np.random.standard_normal(layer_size)\n",
    "        self.size = layer_size\n",
    "        \n",
    "        self.activation_function = activation_function\n",
    "        self.derivated_activation_function = derivated_activation_function\n",
    "        \n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self.dL_dW = None\n",
    "        self.dL_db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward the input vector through the layer\n",
    "        \"\"\"\n",
    "        z = np.dot(x, self.W) + self.b\n",
    "        self.y = self.activation_function(z)\n",
    "        self.x = x # (storing value)\n",
    "        return self.y\n",
    "\n",
    "    def backward(self, dL_dy):\n",
    "        \"\"\"\n",
    "        Back-propagate the loss, computing all the derivatives, storing those with respect to the layer parameters,\n",
    "        and returning the loss with respect to its inputs for further propagation.\n",
    "        Args:\n",
    "            dL_dy (ndarray): The loss derivative with respect to the layer's output (dL/dy = l'_{k+1}).\n",
    "        Returns:\n",
    "            dL_dx (ndarray): The loss derivative with respect to the layer's input (dL/dx).\n",
    "        \"\"\"\n",
    "        dy_dz = self.derivated_activation_function(self.y)  # = f'\n",
    "        dL_dz = (dL_dy * dy_dz) # dL/dz = dL/dy * dy/dz = l'_{k+1} * f'\n",
    "        dz_dw = self.x.T\n",
    "        dz_dx = self.W.T\n",
    "        dz_db = np.ones(dL_dy.shape[0]) # dz/db = d(W.x + b)/db = 0 + db/db = \"ones\"-vector\n",
    "\n",
    "        # Computing the derivatives with respect to the layer's parameters, and storing them for optimization:\n",
    "        self.dL_dW = np.dot(dz_dw, dL_dz)\n",
    "        self.dL_db = np.dot(dz_db, dL_dz)\n",
    "\n",
    "        # Computing the derivative with respect to the input, to be passed to the previous layers (their `dL_dy`):\n",
    "        dL_dx = np.dot(dL_dz, dz_dx)\n",
    "        return dL_dx\n",
    "\n",
    "    def optimize(self, epsilon):\n",
    "        \"\"\"\n",
    "        Optimize the layer's parameters, using the stored derivative values.\n",
    "        Args:\n",
    "            epsilon (float): The learning rate.\n",
    "        \"\"\"\n",
    "        self.W -= epsilon * self.dL_dW\n",
    "        self.b -= epsilon * self.dL_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c34bcd16-6c7e-4a7f-9055-17aeee66ffab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "\n",
    "def sigmoid(x):\n",
    "    y = 1 / (1 + np.exp(-x))\n",
    "    return y\n",
    "\n",
    "def derivated_sigmoid(y):\n",
    "    return y * (1 - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8b3ef14d-ab00-4878-ab97-ac9ebdff20ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function \n",
    "\n",
    "def loss_MSE(pred, target):             # L2/MSE loss \n",
    "    return np.sum(np.square(pred - target)) / pred.shape[0] \n",
    "\n",
    "def derivated_loss_MSE(pred, target):   # L2 derivative \n",
    "    return 2 * (pred - target)\n",
    "\n",
    "def binary_cross_entropy(pred, target):            # cross-entropy loss\n",
    "    return -np.mean(np.multiply(np.log(pred), target) + np.multiply(np.log(1 - pred), (1 - target)))\n",
    "\n",
    "def derivated_binary_cross_entropy(pred, target):  # cross-entropy derivative \n",
    "    return (pred - target) / (pred * (1 - pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "dd8f0403-62b4-4634-8714-5595a4c22789",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNetwork(object):\n",
    "    \"\"\"A simple fully-connected NN.\n",
    "    Args:\n",
    "        num_inputs (int): input vector size \n",
    "        num_outputs (int): output vector size\n",
    "        hidden_layers_sizes (list): A list of sizes for each hidden layer to add to the network\n",
    "        activation_function (callable): The activation function for all the layers\n",
    "        derivated_activation_function (callable): The derivated activation function\n",
    "        loss_function (callable): The loss function to train this network\n",
    "        derivated_loss_function (callable): The derivative of the loss function, for back-propagation\n",
    "    Attributes:\n",
    "        layers (list): The list of layers forming this simple network.\n",
    "        loss_function (callable): The loss function to train this network.\n",
    "        derivated_loss_function (callable): The derivative of the loss function, for back-propagation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_layers_sizes=(64, 32),\n",
    "                 activation_function=sigmoid, derivated_activation_function=derivated_sigmoid,\n",
    "                 loss_function=loss_MSE, derivated_loss_function=derivated_loss_MSE):\n",
    "        super().__init__()\n",
    "        # We build the list of layers composing the network, according to the provided arguments:\n",
    "        layer_sizes = [num_inputs, *hidden_layers_sizes, num_outputs]\n",
    "        self.layers = [\n",
    "            FullyConnectedLayer(layer_sizes[i], layer_sizes[i + 1], \n",
    "                                activation_function, derivated_activation_function)\n",
    "            for i in range(len(layer_sizes) - 1)]\n",
    "\n",
    "        self.loss_function = loss_function\n",
    "        self.derivated_loss_function = derivated_loss_function\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward the input vector through the layers, returning the output vector.\n",
    "        Args:\n",
    "            x (ndarray): The input vector, of shape `(batch_size, num_inputs)`.\n",
    "        Returns:\n",
    "            activation (ndarray): The output activation value, of shape `(batch_size, layer_size)`.\n",
    "        \"\"\"\n",
    "        for layer in self.layers: # from the input layer to the output one\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Compute the output corresponding to input `x`, and return the index of the largest \n",
    "        output value.\n",
    "        Args:\n",
    "            x (ndarray): The input vector, of shape `(1, num_inputs)`.\n",
    "        Returns:\n",
    "            best_class (int): The predicted class ID.\n",
    "        \"\"\"\n",
    "        estimations = self.forward(x)\n",
    "        best_class = np.argmax(estimations)\n",
    "        return best_class\n",
    "\n",
    "    def backward(self, dL_dy):\n",
    "        \"\"\"\n",
    "        Back-propagate the loss hrough the layers (require `forward()` to be called before).\n",
    "        Args:\n",
    "            dL_dy (ndarray): The loss derivative w.r.t. the network's output (dL/dy).\n",
    "        Returns:\n",
    "            dL_dx (ndarray): The loss derivative w.r.t. the network's input (dL/dx).\n",
    "        \"\"\"\n",
    "        for layer in reversed(self.layers): # from the output layer to the input one\n",
    "            dL_dy = layer.backward(dL_dy)\n",
    "        return dL_dy\n",
    "\n",
    "    def optimize(self, epsilon):\n",
    "        \"\"\"\n",
    "        Optimize the network parameters according to the stored gradients (require `backward()`\n",
    "        to be called before).\n",
    "        Args:\n",
    "            epsilon (float): The learning rate.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:             # the order doesn't matter here\n",
    "            layer.optimize(epsilon)\n",
    "\n",
    "    def evaluate_accuracy(self, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Given a dataset and its ground-truth labels, evaluate the current accuracy of the network.\n",
    "        Args:\n",
    "            X_val (ndarray): The input validation dataset.\n",
    "            y_val (ndarray): The corresponding ground-truth validation dataset.\n",
    "        Returns:\n",
    "            accuracy (float): The accuracy of the network \n",
    "                              (= number of correct predictions/dataset size).\n",
    "        \"\"\"\n",
    "        num_corrects = 0\n",
    "        for i in range(len(X_val)):\n",
    "            pred_class = self.predict(X_val[i])\n",
    "            if pred_class == y_val[i]:\n",
    "                num_corrects += 1\n",
    "        return num_corrects / len(X_val)\n",
    "\n",
    "    def train(self, X_train, y_train, X_val=None, y_val=None, \n",
    "              batch_size=32, num_epochs=5, learning_rate=1e-3, print_frequency=20):\n",
    "        \"\"\"\n",
    "        Given a dataset and its ground-truth labels, evaluate the current accuracy of the network.\n",
    "        Args:\n",
    "            X_train (ndarray): The input training dataset.\n",
    "            y_train (ndarray): The corresponding ground-truth training dataset.\n",
    "            X_val (ndarray): The input validation dataset.\n",
    "            y_val (ndarray): The corresponding ground-truth validation dataset.\n",
    "            batch_size (int): The mini-batch size.\n",
    "            num_epochs (int): The number of training epochs i.e. iterations over the whole dataset.\n",
    "            learning_rate (float): The learning rate to scale the derivatives.\n",
    "            print_frequency (int): Frequency to print metrics (in epochs).\n",
    "        Returns:\n",
    "            losses (list): The list of training losses for each epoch.\n",
    "            accuracies (list): The list of validation accuracy values for each epoch.\n",
    "        \"\"\"\n",
    "        num_batches_per_epoch = len(X_train) // batch_size\n",
    "        do_validation = X_val is not None and y_val is not None\n",
    "        losses, accuracies = [], []\n",
    "        for i in range(num_epochs): # for each training epoch\n",
    "            epoch_loss = 0\n",
    "            for b in range(num_batches_per_epoch):  # for each batch composing the dataset\n",
    "                # Get batch:\n",
    "                batch_index_begin = b * batch_size\n",
    "                batch_index_end = batch_index_begin + batch_size\n",
    "                x = X_train[batch_index_begin: batch_index_end]\n",
    "                targets = y_train[batch_index_begin: batch_index_end]\n",
    "                # Optimize on batch:\n",
    "                predictions = y = self.forward(x)  # forward pass\n",
    "                L = self.loss_function(predictions, targets)  # loss computation\n",
    "                dL_dy = self.derivated_loss_function(predictions, targets)  # loss derivation\n",
    "                self.backward(dL_dy)  # back-propagation pass\n",
    "                self.optimize(learning_rate)  # optimization of the NN\n",
    "                epoch_loss += L\n",
    "\n",
    "            # Logging training loss and validation accuracy, to follow the training:\n",
    "            epoch_loss /= num_batches_per_epoch\n",
    "            losses.append(epoch_loss)\n",
    "            if do_validation:\n",
    "                accuracy = self.evaluate_accuracy(X_val, y_val)\n",
    "                accuracies.append(accuracy)\n",
    "            else:\n",
    "                accuracy = np.NaN\n",
    "            if i % print_frequency == 0 or i == (num_epochs - 1):\n",
    "                print(\"Epoch {:4d}: training loss = {:.6f} | val accuracy = {:.2f}%\".format(\n",
    "                    i, epoch_loss, accuracy * 100))\n",
    "        return losses, accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4a3a59-76b4-4856-b388-a822cafc123b",
   "metadata": {},
   "source": [
    "## Implementation with mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "de4f5246-b2e2-44b8-82fd-8f2656aab303",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "254d8026-b9a6-4cab-9afc-4090077d3f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7f14c845-c6e0-481f-b540-a136f301edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "87a0c567-aad9-48b5-a530-2faacc64b7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "953f89b2-7104-44c4-8679-7e8aa108e0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image :\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "True label : 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Image :\")\n",
    "img_idx = np.random.randint(0, X_test.shape[0])\n",
    "plt.imshow(X_test[img_idx], cmap=matplotlib.cm.binary)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(\"--\"*50)\n",
    "print(\"True label :\", y_test[img_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a65f3b4f-4c4e-407c-a882-6f3fae28e3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 28 * 28)\n",
    "X_test = X_test.reshape(-1, 28 * 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "356455c4-7bd6-4ebd-a94e-bd408de48fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a422fbfd-90ea-4d8f-949a-13dc54d0dc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel values between 0 and 255\n"
     ]
    }
   ],
   "source": [
    "print(\"Pixel values between {} and {}\".format(X_train.min(), X_train.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "002ea872-9083-4832-ad1f-bfcf6b8bf0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized pixel values between 0.0 and 1.0\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = X_train / 255., X_test / 255.\n",
    "print(\"Normalized pixel values between {} and {}\".format(X_train.min(), X_train.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "6a11950e-5b31-4c32-a6da-abd812ffddc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.eye(num_classes)[y_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d6e114-ad0e-4105-8e0d-d4770277e811",
   "metadata": {},
   "source": [
    "Data is prepped, let's create our network to fit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "3b9f027d-b0f1-4df5-8d26-c7e05e726a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_classifier = SimpleNetwork(num_inputs=X_train.shape[1], \n",
    "                                 num_outputs=num_classes, \n",
    "                                 hidden_layers_sizes=[64, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "44298c35-ac4a-4f39-9fab-150524d90f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained : training loss = 2.643254 | val accuracy = 11.13%\n"
     ]
    }
   ],
   "source": [
    "predictions = mnist_classifier.forward(X_train)                         # forward pass\n",
    "loss_untrained = mnist_classifier.loss_function(predictions, y_train)   # loss computation\n",
    "\n",
    "accuracy_untrained = mnist_classifier.evaluate_accuracy(X_test, y_test)  # Accuracy\n",
    "print(\"Untrained : training loss = {:.6f} | val accuracy = {:.2f}%\".format(\n",
    "    loss_untrained, accuracy_untrained * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2338238-c960-4edc-b01e-3d88067ca698",
   "metadata": {},
   "source": [
    "Performance before training is as expected, around 10%. All seems normal at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "46dacc91-a270-482b-8d2f-78339f49d8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0: training loss = 0.950690 | val accuracy = 19.70%\n",
      "Epoch   20: training loss = 0.305614 | val accuracy = 78.71%\n",
      "Epoch   40: training loss = 0.187389 | val accuracy = 88.65%\n",
      "Epoch   60: training loss = 0.150658 | val accuracy = 90.54%\n",
      "Epoch   80: training loss = 0.131212 | val accuracy = 91.58%\n",
      "Epoch   99: training loss = 0.118551 | val accuracy = 92.19%\n"
     ]
    }
   ],
   "source": [
    "losses, accuracies = mnist_classifier.train(X_train, y_train, X_test, y_test, \n",
    "                                            batch_size=20, num_epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd12fe5-7735-471c-a19a-7d1f65faf273",
   "metadata": {},
   "source": [
    "80% accuracy after 20 epochs, almost 90 after 40 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979d4ce3-3403-4b1e-9f5f-2cd507a7920c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
